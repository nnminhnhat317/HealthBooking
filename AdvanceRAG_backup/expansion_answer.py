from helper_utils import project_embeddings, word_wrap
from pypdf import PdfReader
import os
from openai import OpenAI
from dotenv import load_dotenv
from pypdf import PdfReader
import umap

load_dotenv()

openai_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_key)

reader =    ("data/advanceRAG/microsoft-annual-report.pdf")
pdf_texts = [p.extract_text().strip() for p in reader.pages]  # Tách text từng page, .strip() xóa khoảng trắng, ký tự xuống dòng đầu và cuối của dòng, lưu vào list
#op: ["Page 1 content", "", "Page 3 content", None, " "]

# Filter the empty strings
pdf_texts = [text for text in pdf_texts if text] #op: ["Page 1 content", "Page 3 content"]
# print(
#     word_wrap(
#         pdf_texts[0],
#         width=100,
#     )
# )

# split the text into smaller chunks

from langchain_text_splitters import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter

character_splitter = RecursiveCharacterTextSplitter( #Tách đoạn văn bản thành các đoạn nhỏ hơn dựa trên ký tự
    separators=["\n\n", "\n", ". ", " ", ""], chunk_size=1000, chunk_overlap=0 # khi gần ngưỡng chunk_size thì xác định các vị trí lý tưởng để cắt văn bản thành các chunk nhỏ.
)
character_split_texts = character_splitter.split_text("\n\n".join(pdf_texts)) # nối tất cả các trang pdf thành 1 chuỗi dài,op: ["Page 1 content\n\nPage 2 content\n\n..."]

# print(word_wrap(character_split_texts[10]))
# print(f"\nTotal chunks: {len(character_split_texts)}")

token_splitter = SentenceTransformersTokenTextSplitter( #Tách đoạn văn bản thành các đoạn nhỏ hơn dựa trên số token
    chunk_overlap=0, tokens_per_chunk=256
)
token_split_texts = []
for text in character_split_texts:
    token_split_texts += token_splitter.split_text(text) #op: for ["A","B"]+= ["C","D"] -> ["A","B","C","D"]
    # op: .split_text()
    #   [
    #     "Page 1 content ... (≈256 token)",
    #     "Some next content ... (≈256 token)",
    #     "Remaining content ... (≈256 token)",
    #   ]

# print(word_wrap(token_split_texts[10]))
# print(f"\nTotal chunks: {len(token_split_texts)}")


import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

embedding_function = SentenceTransformerEmbeddingFunction()
# print(embedding_function([token_split_texts[10]]))

chroma_client = chromadb.Client() # collection đc lưu tạm thời trong RAM
chroma_collection = chroma_client.create_collection(
    "microsoft-collection", embedding_function=embedding_function
)

# extract the embeddings of the token_split_texts
ids = [str(i) for i in range(len(token_split_texts))] #op: ["0","1","2",...]
chroma_collection.add(ids=ids, documents=token_split_texts)
chroma_collection.count()

query = "What was the total revenue for the year?"


# results = chroma_collection.query(query_texts=[query], n_results=5) # test truy vấn 5 document tương tự nhất với query 
# retrieved_documents = results["documents"][0]

    # for document in retrieved_documents:
    #     print(word_wrap(document))
    #     print("\n")

            # Augment the query with a hypothetical answer generated by an LLM
def augment_query_generated(query, model="gpt-3.5-turbo"):
    prompt = """You are a helpful expert financial research assistant. 
   Provide an example answer to the given question, that might be found in a document like an annual report."""
    messages = [
        {
            "role": "system",
            "content": prompt,
        },
        {   "role": "user",
            "content": query
        },
    ]

    response = client.chat.completions.create(
        model=model,
        messages=messages,
    )
    content = response.choices[0].message.content
    return content


original_query = "What was the total profit for the year, and how does it compare to the previous year?"
hypothetical_answer = augment_query_generated(original_query)

joint_query = f"{original_query} {hypothetical_answer}"
print("[joint_query:]\n",word_wrap(joint_query))

results = chroma_collection.query(
    query_texts=joint_query, n_results=5, include=["documents", "embeddings"]
)
retrieved_documents = results["documents"][0]
print("\n[Final Retrieved Documents:]\n")

    # for doc in retrieved_documents:
    #     print(word_wrap(doc))
    #     print("")
                    # Trực quan hóa embeddings bằng UMAP và matplotlib
# embeddings = chroma_collection.get(include=["embeddings"])["embeddings"]
# umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)
# projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)


# retrieved_embeddings = results["embeddings"][0]
# original_query_embedding = embedding_function([original_query])
# augmented_query_embedding = embedding_function([joint_query])

# projected_original_query_embedding = project_embeddings(
#     original_query_embedding, umap_transform
# )
# projected_augmented_query_embedding = project_embeddings(
#     augmented_query_embedding, umap_transform
# )
# projected_retrieved_embeddings = project_embeddings(
#     retrieved_embeddings, umap_transform
# )

# import matplotlib.pyplot as plt

# # Plot the projected query and retrieved documents in the embedding space
# plt.figure()

# plt.scatter(
#     projected_dataset_embeddings[:, 0],
#     projected_dataset_embeddings[:, 1],
#     s=10,
#     color="gray",
# )
# plt.scatter(
#     projected_retrieved_embeddings[:, 0],
#     projected_retrieved_embeddings[:, 1],
#     s=100,
#     facecolors="none",
#     edgecolors="g",
# )
# plt.scatter(
#     projected_original_query_embedding[:, 0],
#     projected_original_query_embedding[:, 1],
#     s=150,
#     marker="X",
#     color="r",
# )
# plt.scatter(
#     projected_augmented_query_embedding[:, 0],
#     projected_augmented_query_embedding[:, 1],
#     s=150,
#     marker="X",
#     color="orange",
# )

# plt.gca().set_aspect("equal", "datalim")
# plt.title(f"{original_query}")
# plt.axis("off")
# plt.show()  # display the plot